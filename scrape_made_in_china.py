import requests
from bs4 import BeautifulSoup
import time
import random
from pathlib import Path

# –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö User-Agent –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —Ä–∞–∑–Ω—ã—Ö –±—Ä–∞—É–∑–µ—Ä–æ–≤
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 Safari/605.1.15",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 15_2 like Mac OS X) AppleWebKit/605.1.15 Mobile/15E148",
]

def fetch_page(url, retries=3):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ø–æ–≤—Ç–æ—Ä–∞–º–∏ –∏ —Å–ª—É—á–∞–π–Ω—ã–º User-Agent.
    """
    for attempt in range(retries):
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://www.google.com/',
        }
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.text
            elif response.status_code in [429, 503]:
                wait = 2 ** attempt
                print(f"[WARN] –ö–æ–¥ {response.status_code} ‚Äî retry {attempt+1}/{retries} after {wait}s")
                time.sleep(wait)
            else:
                print(f"[ERROR] –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å: {response.status_code}")
                break
        except requests.RequestException as e:
            print(f"[ERROR] –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}")
            time.sleep(2)
    return None

def scrape_made_in_china_companies(category_url, pages=5, delay=1.0):
    companies = set()
    for page in range(1, pages + 1):
        url = f"{category_url}?page={page}"
        print(f"Fetching: {url}")

        html = fetch_page(url)
        if not html:
            print(f"[FAIL] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
            break

        soup = BeautifulSoup(html, 'html.parser')

        for item in soup.select('a.compnay-name span'):
            companies.add(item.get_text(strip=True))

        jitter = random.uniform(0.5, delay)
        time.sleep(delay + jitter)

    return companies

if __name__ == "__main__":
    BASE_DIR = Path(__file__).resolve().parent
    OUTPUT_PATH = BASE_DIR / "companies.txt"

    category = "https://www.made-in-china.com/Consumer-Electronics-Catalog/Mobile-Phone.html"
    print("–°–æ–±–∏—Ä–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏...")
    company_names = scrape_made_in_china_companies(category, pages=10, delay=1.5)

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        for name in sorted(company_names):
            f.write(name + "\n")

    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ–±—Ä–∞–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(company_names)}")
    print(f"üìÑ –°–ø–∏—Å–æ–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {OUTPUT_PATH.name}")
